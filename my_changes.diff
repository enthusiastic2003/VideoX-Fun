diff --git a/.gitignore b/.gitignore
index 5627e45..bdbcdb0 100644
--- a/.gitignore
+++ b/.gitignore
@@ -168,3 +168,7 @@ cython_debug/
 #  and can be added to the global gitignore or merged into this file.  For a more nuclear
 #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
 #.idea/
+
+
+## Personal Additions
+cogv/
diff --git a/debug_control_frames.py b/debug_control_frames.py
new file mode 100644
index 0000000..a50ae5d
--- /dev/null
+++ b/debug_control_frames.py
@@ -0,0 +1,86 @@
+import os
+import torch
+import cv2
+import numpy as np
+from PIL import Image
+
+# Check the control video file
+control_video_path = "asset/src_video_depth_49_896x512_8fps.mp4"
+
+if os.path.exists(control_video_path):
+    cap = cv2.VideoCapture(control_video_path)
+    
+    # Get video properties
+    fps = cap.get(cv2.CAP_PROP_FPS)
+    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
+    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
+    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
+    
+    print(f"Control Video Properties:")
+    print(f"  FPS: {fps}")
+    print(f"  Total Frames: {frame_count}")
+    print(f"  Resolution: {width}x{height} (W x H)")
+    
+    cap.release()
+    
+    # Now simulate what get_video_to_video_latent does
+    sample_size = [512, 896]  # height, width
+    video_length = 49
+    fps_input = 8
+    
+    print(f"\nProcessing Parameters:")
+    print(f"  Sample Size: {sample_size} (H x W)")
+    print(f"  Video Length: {video_length}")
+    print(f"  Desired FPS: {fps_input}")
+    
+    # Simulate frame reading
+    cap = cv2.VideoCapture(control_video_path)
+    original_fps = cap.get(cv2.CAP_PROP_FPS)
+    frame_skip = 1 if fps_input is None else max(1, int(original_fps // fps_input))
+    
+    print(f"  Original FPS: {original_fps}")
+    print(f"  Frame Skip: {frame_skip}")
+    
+    input_video = []
+    frame_count = 0
+    
+    while True:
+        ret, frame = cap.read()
+        if not ret:
+            break
+        
+        if frame_count % frame_skip == 0:
+            frame = cv2.resize(frame, (sample_size[1], sample_size[0]))
+            input_video.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
+        
+        frame_count += 1
+    
+    cap.release()
+    
+    # Convert to tensor (same as get_video_to_video_latent)
+    input_video = torch.from_numpy(np.array(input_video))[:video_length]
+    print(f"\nAfter reading and truncating:")
+    print(f"  Input video shape (F, H, W, C): {input_video.shape}")
+    
+    # Permute to (C, F, H, W) and add batch dimension
+    input_video = input_video.permute([3, 0, 1, 2]).unsqueeze(0) / 255
+    print(f"  After permute and batch add (B, C, F, H, W): {input_video.shape}")
+    
+    # Now check VAE encoding expectations
+    print(f"\nVAE Encoding Expectations:")
+    print(f"  For video_length={video_length} frames with temporal_compression_ratio=4:")
+    
+    # From predict script
+    video_length_adjusted = int((video_length - 1) // 4 * 4) + 1 if video_length != 1 else 1
+    latent_frames = (video_length_adjusted - 1) // 4 + 1
+    
+    print(f"    Adjusted video_length: {video_length_adjusted}")
+    print(f"    Expected latent_frames: {latent_frames}")
+    
+else:
+    print(f"Control video file not found: {control_video_path}")
+    print(f"Current directory: {os.getcwd()}")
+    print(f"Files in asset/:")
+    if os.path.exists("asset"):
+        for f in os.listdir("asset"):
+            print(f"  {f}")
diff --git a/examples/cogvideox_fun/predict_v2v_control.py b/examples/cogvideox_fun/predict_v2v_control.py
index 1193d33..e9ea23f 100755
--- a/examples/cogvideox_fun/predict_v2v_control.py
+++ b/examples/cogvideox_fun/predict_v2v_control.py
@@ -55,7 +55,7 @@ fsdp_text_encoder   = True
 compile_dit         = False
 
 # model path
-model_name          = "models/Diffusion_Transformer/CogVideoX-Fun-V1.1-2b-Pose"
+model_name          = "models/Diffusion_Transformer/CogVideoX-Fun-V1.1-5b-Control"
 
 # Choose the sampler in "Euler" "Euler A" "DPM++" "PNDM" "DDIM_Cog" and "DDIM_Origin"
 sampler_name        = "DDIM_Origin"
@@ -65,19 +65,19 @@ transformer_path    = None
 vae_path            = None
 lora_path           = None
 # Other params
-sample_size         = [672, 384]
+sample_size         = [512, 896]  # height, width
 # V1.0 and V1.1 support up to 49 frames of video generation,
 # while V1.5 supports up to 85 frames.  
 video_length        = 49
-fps                 = 8
+fps                 = 30
 
 # Use torch.float16 if GPU does not support torch.bfloat16
 # ome graphics cards, such as v100, 2080ti, do not support torch.bfloat16
 weight_dtype            = torch.bfloat16
-control_video           = "asset/pose.mp4"
+control_video           = "asset/depth_map_video_60frames_896x512.mp4"
 
 # prompts
-prompt                  = "A young woman with beautiful face, dressed in white, is moving her body. "
+prompt                  = "A photorealistic woman performing yoga moves carefully in a brightly lit room, sunlight coming through the window illuminates her clothes as she moves. She is wearing black yoga pants with a black tank top, her hair is loose, her backside is fully visible, and the image only shows her backside."
 negative_prompt         = "The video is not of a high quality, it has a low resolution. Watermark present in each frame. The background is solid. Strange body and strange trajectory. Distortion. "
 guidance_scale          = 6.0
 seed                    = 43
@@ -146,6 +146,9 @@ scheduler = Chosen_Scheduler.from_pretrained(
     subfolder="scheduler"
 )
 
+# Get Latent for Video
+
+
 pipeline = CogVideoXFunControlPipeline(
     vae=vae,
     tokenizer=tokenizer,
diff --git a/examples/cogvideox_fun/predict_v2v_control_latents.py b/examples/cogvideox_fun/predict_v2v_control_latents.py
new file mode 100755
index 0000000..11ecd00
--- /dev/null
+++ b/examples/cogvideox_fun/predict_v2v_control_latents.py
@@ -0,0 +1,244 @@
+import os
+import sys
+
+import cv2
+import numpy as np
+import torch
+from diffusers import (CogVideoXDDIMScheduler, DDIMScheduler,
+                       DPMSolverMultistepScheduler,
+                       EulerAncestralDiscreteScheduler, EulerDiscreteScheduler,
+                       PNDMScheduler)
+from PIL import Image
+from transformers import T5EncoderModel
+
+current_file_path = os.path.abspath(__file__)
+project_roots = [os.path.dirname(current_file_path), os.path.dirname(os.path.dirname(current_file_path)), os.path.dirname(os.path.dirname(os.path.dirname(current_file_path)))]
+for project_root in project_roots:
+    sys.path.insert(0, project_root) if project_root not in sys.path else None
+
+from videox_fun.models import (AutoencoderKLCogVideoX,
+                              CogVideoXTransformer3DModel, T5EncoderModel,
+                              T5Tokenizer)
+from videox_fun.pipeline import (CogVideoXFunControlPipeline,
+                                CogVideoXFunInpaintPipeline)
+from videox_fun.utils.fp8_optimization import (convert_model_weight_to_float8, replace_parameters_by_name,
+                                              convert_weight_dtype_wrapper)
+from videox_fun.utils.lora_utils import merge_lora, unmerge_lora
+from videox_fun.utils.utils import get_video_to_video_latent, save_videos_grid
+from videox_fun.dist import set_multi_gpus_devices, shard_model
+from diffusers.image_processor import VaeImageProcessor
+
+# GPU memory mode, which can be chosen in [model_full_load, model_full_load_and_qfloat8, model_cpu_offload, model_cpu_offload_and_qfloat8, sequential_cpu_offload].
+# model_full_load means that the entire model will be moved to the GPU.
+# 
+# model_full_load_and_qfloat8 means that the entire model will be moved to the GPU,
+# and the transformer model has been quantized to float8, which can save more GPU memory. 
+# 
+# model_cpu_offload means that the entire model will be moved to the CPU after use, which can save some GPU memory.
+# 
+# model_cpu_offload_and_qfloat8 indicates that the entire model will be moved to the CPU after use, 
+# and the transformer model has been quantized to float8, which can save more GPU memory. 
+# 
+# sequential_cpu_offload means that each layer of the model will be moved to the CPU after use, 
+# resulting in slower speeds but saving a large amount of GPU memory.
+GPU_memory_mode     = "model_cpu_offload_and_qfloat8"
+# Multi GPUs config
+# Please ensure that the product of ulysses_degree and ring_degree equals the number of GPUs used. 
+# For example, if you are using 8 GPUs, you can set ulysses_degree = 2 and ring_degree = 4.
+# If you are using 1 GPU, you can set ulysses_degree = 1 and ring_degree = 1.
+ulysses_degree      = 1
+ring_degree         = 1
+# Use FSDP to save more GPU memory in multi gpus.
+fsdp_dit            = False
+fsdp_text_encoder   = True
+# Compile will give a speedup in fixed resolution and need a little GPU memory. 
+# The compile_dit is not compatible with the fsdp_dit and sequential_cpu_offload.
+compile_dit         = False
+
+# model path
+model_name          = "models/Diffusion_Transformer/CogVideoX-Fun-V1.1-5b-Control"
+
+# Choose the sampler in "Euler" "Euler A" "DPM++" "PNDM" "DDIM_Cog" and "DDIM_Origin"
+sampler_name        = "DDIM_Origin"
+
+# Load pretrained model if need
+transformer_path    = None
+vae_path            = None
+lora_path           = None
+# Other params
+sample_size         = [512, 896]  # height, width
+# V1.0 and V1.1 support up to 49 frames of video generation,
+# while V1.5 supports up to 85 frames.  
+video_length        = 49
+fps                 = 30
+
+# Use torch.float16 if GPU does not support torch.bfloat16
+# ome graphics cards, such as v100, 2080ti, do not support torch.bfloat16
+weight_dtype            = torch.bfloat16
+control_video           = "asset/depth_map_video_60frames_896x512.mp4"
+
+# prompts
+prompt                  = "A photorealistic woman performing yoga moves carefully in a brightly lit room, sunlight coming through the window illuminates her clothes as she moves. She is wearing black yoga pants with a black tank top, her hair is loose, her backside is fully visible, and the image only shows her backside."
+negative_prompt         = "The video is not of a high quality, it has a low resolution. Watermark present in each frame. The background is solid. Strange body and strange trajectory. Distortion. "
+guidance_scale          = 6.0
+seed                    = 43
+num_inference_steps     = 50
+lora_weight             = 0.55
+save_path               = "samples/cogvideox-fun-videos_control"
+
+device = set_multi_gpus_devices(ulysses_degree, ring_degree)
+
+transformer = CogVideoXTransformer3DModel.from_pretrained(
+    model_name, 
+    subfolder="transformer",
+    low_cpu_mem_usage=True,
+    torch_dtype=weight_dtype,
+).to(weight_dtype)
+
+if transformer_path is not None:
+    print(f"From checkpoint: {transformer_path}")
+    if transformer_path.endswith("safetensors"):
+        from safetensors.torch import load_file, safe_open
+        state_dict = load_file(transformer_path)
+    else:
+        state_dict = torch.load(transformer_path, map_location="cpu")
+    state_dict = state_dict["state_dict"] if "state_dict" in state_dict else state_dict
+
+    m, u = transformer.load_state_dict(state_dict, strict=False)
+    print(f"missing keys: {len(m)}, unexpected keys: {len(u)}")
+
+# Get Vae
+vae = AutoencoderKLCogVideoX.from_pretrained(
+    model_name, 
+    subfolder="vae"
+).to(weight_dtype)
+
+if vae_path is not None:
+    print(f"From checkpoint: {vae_path}")
+    if vae_path.endswith("safetensors"):
+        from safetensors.torch import load_file, safe_open
+        state_dict = load_file(vae_path)
+    else:
+        state_dict = torch.load(vae_path, map_location="cpu")
+    state_dict = state_dict["state_dict"] if "state_dict" in state_dict else state_dict
+
+    m, u = vae.load_state_dict(state_dict, strict=False)
+    print(f"missing keys: {len(m)}, unexpected keys: {len(u)}")
+
+# Get tokenizer and text_encoder
+tokenizer = T5Tokenizer.from_pretrained(
+    model_name, subfolder="tokenizer"
+)
+text_encoder = T5EncoderModel.from_pretrained(
+    model_name, subfolder="text_encoder", torch_dtype=weight_dtype
+)
+
+# Get Scheduler
+Chosen_Scheduler = scheduler_dict = {
+    "Euler": EulerDiscreteScheduler,
+    "Euler A": EulerAncestralDiscreteScheduler,
+    "DPM++": DPMSolverMultistepScheduler, 
+    "PNDM": PNDMScheduler,
+    "DDIM_Cog": CogVideoXDDIMScheduler,
+    "DDIM_Origin": DDIMScheduler,
+}[sampler_name]
+scheduler = Chosen_Scheduler.from_pretrained(
+    model_name, 
+    subfolder="scheduler"
+)
+
+# Get Latent for Video
+
+
+pipeline = CogVideoXFunControlPipeline(
+    vae=vae,
+    tokenizer=tokenizer,
+    text_encoder=text_encoder,
+    transformer=transformer,
+    scheduler=scheduler,
+)
+if ulysses_degree > 1 or ring_degree > 1:
+    from functools import partial
+    transformer.enable_multi_gpus_inference()
+    if fsdp_dit:
+        shard_fn = partial(shard_model, device_id=device, param_dtype=weight_dtype)
+        pipeline.transformer = shard_fn(pipeline.transformer)
+        print("Add FSDP DIT")
+    if fsdp_text_encoder:
+        shard_fn = partial(shard_model, device_id=device, param_dtype=weight_dtype)
+        pipeline.text_encoder = shard_fn(pipeline.text_encoder)
+        print("Add FSDP TEXT ENCODER")
+
+if compile_dit:
+    for i in range(len(pipeline.transformer.transformer_blocks)):
+        pipeline.transformer.transformer_blocks[i] = torch.compile(pipeline.transformer.transformer_blocks[i])
+    print("Add Compile")
+
+if GPU_memory_mode == "sequential_cpu_offload":
+    pipeline.enable_sequential_cpu_offload(device=device)
+elif GPU_memory_mode == "model_cpu_offload_and_qfloat8":
+    convert_model_weight_to_float8(transformer, exclude_module_name=[], device=device)
+    convert_weight_dtype_wrapper(transformer, weight_dtype)
+    pipeline.enable_model_cpu_offload(device=device)
+elif GPU_memory_mode == "model_cpu_offload":
+    pipeline.enable_model_cpu_offload(device=device)
+elif GPU_memory_mode == "model_full_load_and_qfloat8":
+    convert_model_weight_to_float8(transformer, exclude_module_name=[], device=device)
+    convert_weight_dtype_wrapper(transformer, weight_dtype)
+    pipeline.to(device=device)
+else:
+    pipeline.to(device=device)
+
+generator = torch.Generator(device=device).manual_seed(seed)
+
+if lora_path is not None:
+    pipeline = merge_lora(pipeline, lora_path, lora_weight, device=device, dtype=weight_dtype)
+
+video_length = int((video_length - 1) // vae.config.temporal_compression_ratio * vae.config.temporal_compression_ratio) + 1 if video_length != 1 else 1
+latent_frames = (video_length - 1) // vae.config.temporal_compression_ratio + 1
+if video_length != 1 and transformer.config.patch_size_t is not None and latent_frames % transformer.config.patch_size_t != 0:
+    additional_frames = transformer.config.patch_size_t - latent_frames % transformer.config.patch_size_t
+    video_length += additional_frames * vae.config.temporal_compression_ratio
+input_video, input_video_mask, ref_image, clip_image = get_video_to_video_latent(control_video, video_length=video_length, sample_size=sample_size, fps=fps)
+
+with torch.no_grad():
+    sample = pipeline(
+        prompt, 
+        num_frames = video_length,
+        negative_prompt = negative_prompt,
+        height      = sample_size[0],
+        width       = sample_size[1],
+        generator   = generator,
+        guidance_scale = guidance_scale,
+        num_inference_steps = num_inference_steps,
+
+        control_video = input_video,
+    ).videos
+
+if lora_path is not None:
+    pipeline = unmerge_lora(pipeline, lora_path, lora_weight, device=device, dtype=weight_dtype)
+
+def save_results():
+    if not os.path.exists(save_path):
+        os.makedirs(save_path, exist_ok=True)
+
+    index = len([path for path in os.listdir(save_path)]) + 1
+    prefix = str(index).zfill(8)
+    if video_length == 1:
+        video_path = os.path.join(save_path, prefix + ".png")
+
+        image = sample[0, :, 0]
+        image = image.transpose(0, 1).transpose(1, 2)
+        image = (image * 255).numpy().astype(np.uint8)
+        image = Image.fromarray(image)
+        image.save(video_path)
+    else:
+        video_path = os.path.join(save_path, prefix + ".mp4")
+        save_videos_grid(sample, video_path, fps=fps)
+
+if ulysses_degree * ring_degree > 1:
+    import torch.distributed as dist
+    if dist.get_rank() == 0:
+        save_results()
+else:
+    save_results()
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index ff393ff..5804efb 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,28 +1,37 @@
-Pillow
-einops
+# Core
+numpy==1.26.4
+torch==2.2.0
+torchvision
 safetensors
-timm
-tomesd
-librosa
-torch>=2.1.2
-torchdiffeq
-torchsde
-decord
-datasets
-numpy
-scikit-image
-opencv-python
+einops
 omegaconf
 SentencePiece
-albumentations
+
+# Diffusion stack (PINNED)
+diffusers==0.25.0
+huggingface_hub==0.19.4
+transformers==4.36.2
+accelerate==0.24.1
+
+# Video / image
+opencv-python-headless==4.8.1.78
+scikit-image
 imageio[ffmpeg]
 imageio[pyav]
+decord
+albumentations
+Pillow
+
+# Models / utils
+timm
+tomesd
+torchdiffeq
+torchsde
+onnxruntime
 tensorboard
 beautifulsoup4
 ftfy
 func_timeout
-onnxruntime
-accelerate>=0.25.0
-gradio>=3.41.2
-diffusers>=0.30.1
-transformers>=4.46.2
+
+# Optional UI (SAFE version)
+gradio==3.50.2
\ No newline at end of file
diff --git a/videox_fun/pipeline/pipeline_cogvideox_fun_control.py b/videox_fun/pipeline/pipeline_cogvideox_fun_control.py
index e91df20..c8294fa 100644
--- a/videox_fun/pipeline/pipeline_cogvideox_fun_control.py
+++ b/videox_fun/pipeline/pipeline_cogvideox_fun_control.py
@@ -840,6 +840,7 @@ class CogVideoXFunControlPipeline(DiffusionPipeline):
             generator,
             latents,
         )
+        print("Latents Shape after preparation: ", latents.shape)
         if comfyui_progressbar:
             pbar.update(1)
 
@@ -857,8 +858,10 @@ class CogVideoXFunControlPipeline(DiffusionPipeline):
         control_video_latents_input = (
             torch.cat([control_video_latents] * 2) if do_classifier_free_guidance else control_video_latents
         )
+        print("Control Video Shape after encoding: ", control_video_latents_input.shape)
         control_latents = rearrange(control_video_latents_input, "b c f h w -> b f c h w")
-
+        print("Control Latents Shape after rearrange: ", control_latents.shape)
+        exit(0)
         if comfyui_progressbar:
             pbar.update(1)
 
